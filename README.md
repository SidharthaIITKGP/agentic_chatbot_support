# Agentic Customer Support Chatbot

A sophisticated customer support agent powered by **LangGraph ReAct pattern with LLM reasoning**, combining intelligent LLM-driven decision making, tool execution, and RAG (Retrieval-Augmented Generation) for intelligent query handling.

## ğŸŒŸ Features

### True LLM-Powered ReAct Agent
- **LLM Reasoning**: Language model thinks through problems autonomously
- **Tool Selection**: LLM decides which tools to use and when
- **Adaptive Learning**: Observes results and adapts strategy dynamically
- **Natural Conversation**: No hardcoded rules - pure AI reasoning
- **Transparent**: Full reasoning traces show LLM thought process

### Capabilities
- âœ… Order status tracking
- âœ… Refund status inquiries
- âœ… Product availability checks
- âœ… Policy questions (return, refund, delivery, charges, etc.)
- âœ… Context-aware conversation memory
- âœ… Multi-step reasoning and tool orchestration
- âœ… Intelligent query understanding

### Technology Stack
- **LangGraph**: ReAct agent orchestration with `create_react_agent`
- **LangChain**: LLM integration and tool framework
- **Groq**: Fast inference with `llama-3.1-8b-instant` model
- **RAG**: FAISS vector store with HuggingFace embeddings (all-MiniLM-L6-v2)
- **Streamlit**: Clean chat interface with optional reasoning display
- **UV**: Modern Python package management

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/SidharthaIITKGP/agentic_chatbot_support.git
cd "agentic_chatbot_support"

# Install dependencies (using uv)
uv sync
```

### Environment Setup

Create a `.env` file in the project root:
```bash
GROQ_API_KEY=your_groq_api_key_here
```

Get your free Groq API key from: https://console.groq.com/

### Setup Policy Documents

Ingest policy documents into the vector store:

```bash
uv run python src/rag/ingest_policies.py
```

### Run the Agent

**CLI Mode**:
```bash
uv run python -m src.agent.llm_agent "Where is my order 98762?"
```

**Streamlit UI** (Recommended):
```bash
uv run streamlit run src/ui/streamlit_app.py --server.port 8503
```

Open http://localhost:8503 in your browser.

**Enable reasoning traces** by checking "Show ReAct Reasoning" in the sidebar!

## ğŸ“Š LLM ReAct in Action

```
User: Where is my order 98762 and what if it's delayed?

LLM Thought Process:
  ğŸ’­ "I need to check the order status first. Let me use the 
      get_order_status tool with ID 98762."
      
  ğŸ¬ Action: get_order_status_tool(order_id="98762")
  
  ğŸ‘ï¸ Observation: {order_id: 98762, status: "out for delivery", 
                   expected_delivery: "2025-01-24"}
  
  ğŸ’­ "Good, the order is on the way. Now they asked about delays,
      so I should search the policy documents for delivery delay 
      information."
      
  ğŸ¬ Action: search_policy_documents_tool(query="delivery delay policy")
  
  ğŸ‘ï¸ Observation: Retrieved policy: "If delayed beyond 3 days of 
                   expected delivery, customers eligible for delivery 
                   fee refunds..."
  
  ğŸ’­ "Perfect! I have both the order status and the delay policy.
      I can now compose a complete answer."

Final Answer: Your order 98762 is currently out for delivery, 
expected on January 24, 2025. If it's delayed beyond 3 days of 
the expected date, you'll be eligible for a delivery fee refund 
according to our delivery delay policy...
```

## ğŸ§ª Testing

### Comprehensive LLM Agent Tests
```bash
uv run python test_llm_agent.py
```

Tests all capabilities:
- Order status queries (with/without ID)
- Refund status checks
- Product availability
- Policy questions
- Complex multi-step reasoning

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ .env                         # API keys (GROQ_API_KEY)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agent/
â”‚   â”‚   â”œâ”€â”€ llm_agent.py        # LLM-powered ReAct agent (main)
â”‚   â”‚   â”œâ”€â”€ llm_tools.py        # LangChain tool wrappers
â”‚   â”‚   â””â”€â”€ memory.py           # Persistent conversation memory
â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”œâ”€â”€ embeddings.py       # HuggingFace embeddings
â”‚   â”‚   â”œâ”€â”€ ingest_policies.py  # Policy document ingestion
â”‚   â”‚   â””â”€â”€ retriever.py        # FAISS vector store retriever
â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â”œâ”€â”€ tools.py            # Order, refund, inventory APIs
â”‚   â”‚   â”œâ”€â”€ orders.json         # Mock order data
â”‚   â”‚   â”œâ”€â”€ refunds.json        # Mock refund data
â”‚   â”‚   â””â”€â”€ inventory.json      # Mock inventory data
â”‚   â””â”€â”€ ui/
â”‚       â””â”€â”€ streamlit_app.py    # Chat interface with reasoning display
â”œâ”€â”€ Policy/                      # Policy documents for RAG
â”‚   â”œâ”€â”€ return_policy.txt
â”‚   â”œâ”€â”€ refund_policy.txt
â”‚   â”œâ”€â”€ delivery_delay_policy.txt
â”‚   â”œâ”€â”€ payment_policy.txt
â”‚   â”œâ”€â”€ charges_policy.txt
â”‚   â”œâ”€â”€ cancellation_policy.txt
â”‚   â””â”€â”€ replacement_damage_policy.txt
â”œâ”€â”€ test_llm_agent.py            # Comprehensive test suite
â””â”€â”€ README.md
```

## ğŸ¯ LLM Agent Architecture

```
User Query
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLM ReAct Agent                â”‚
â”‚  (LangGraph create_react_agent) â”‚
â”‚                                 â”‚
â”‚  LLM thinks â†’ decides action    â”‚
â”‚       â†“                         â”‚
â”‚  Tool execution or finish       â”‚
â”‚       â†“                         â”‚
â”‚  LLM observes result            â”‚
â”‚       â†“                         â”‚
â”‚  Loop until LLM decides done    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Final Answer
    â†“
Return to User
```

### Available Tools
1. **get_order_status_tool** - Track order status and delivery info
2. **get_refund_status_tool** - Check refund processing status
3. **check_product_availability_tool** - Verify product stock
4. **search_policy_documents_tool** - RAG-powered policy search

### Key Advantages Over Rule-Based Systems
- âœ… No hardcoded if/else logic
- âœ… LLM decides tool usage dynamically
- âœ… Handles novel queries without code changes
- âœ… Natural multi-step reasoning
- âœ… Adapts to conversation context
- âœ… True AI-powered decision making

## ğŸ”§ Configuration

### LLM Model
Edit `src/agent/llm_agent.py`:
```python
MODEL_NAME = "llama-3.1-8b-instant"  # Groq model
TEMPERATURE = 0.1  # Lower = more focused
```

### RAG Parameters
Edit `src/rag/retriever.py`:
```python
retrieve_policy(query, fetch_k=10, top_k=3, alpha=0.85)
```

### Memory Settings
Memory stored in `src/logs/agent_memory_*.json` with LangGraph checkpointing.

## ğŸ“š Documentation

- **[README.md](README.md)**: This file - setup and usage
- **[test_llm_agent.py](test_llm_agent.py)**: Comprehensive test examples

## ğŸ” Use Cases

### Order Tracking
```
User: Where is my order?
LLM: I need the order ID to look that up. Could you provide it?
User: 98762
LLM: [Uses get_order_status_tool] Order 98762 is out for delivery...
```

### Policy Questions
```
User: What is your return policy?
LLM: [Uses search_policy_documents_tool with query "return policy"]
     Based on our return policy, you can return items within 30 days...
```

### Complex Multi-Step Queries
```
User: Check order 98762 and tell me about delivery delays
LLM: [Step 1: Uses get_order_status_tool]
     [Step 2: Uses search_policy_documents_tool for delay policy]
     Your order is out for delivery. If delayed beyond 3 days...
```

## ğŸ¤ Contributing

This is an educational/demonstration project showcasing LLM-powered ReAct pattern with LangGraph.

## ğŸ“ License

MIT License

## ğŸ™ Acknowledgments

- LangGraph for the graph-based orchestration framework
- LangChain for LLM integration and tool framework
- Groq for fast inference
- The ReAct paper: [Yao et al., 2023](https://arxiv.org/abs/2210.03629)

---

**Built with â¤ï¸ using LLM-Powered LangGraph ReAct Pattern**
